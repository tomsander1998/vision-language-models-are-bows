{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/checkpoint/tomsander/data\"\n",
    "import sys\n",
    "sys.path.insert(0, '/private/home/tomsander/vision-language-models-are-bows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_zoo import VG_Relation, VG_Attribution\n",
    "import torch\n",
    "vgr_dataset = VG_Relation(image_preprocess=None, download=True, root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vgr_dataset[0]['image_options'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[111, 110, 109,  ..., 177, 171, 187],\n",
      "         [116, 114, 114,  ..., 175, 178, 175],\n",
      "         [123, 121, 118,  ..., 170, 173, 170],\n",
      "         ...,\n",
      "         [  5,   6,   5,  ...,   8,  10,   7],\n",
      "         [  4,   3,   5,  ...,   7,  12,   9],\n",
      "         [  4,   5,   6,  ...,   8,   7,   7]],\n",
      "\n",
      "        [[104, 103, 102,  ..., 177, 173, 188],\n",
      "         [108, 106, 106,  ..., 178, 183, 179],\n",
      "         [112, 110, 107,  ..., 173, 179, 176],\n",
      "         ...,\n",
      "         [  5,   7,   5,  ...,  10,  12,   9],\n",
      "         [  5,   4,   6,  ...,   9,  14,  11],\n",
      "         [  4,   6,   6,  ...,  10,   9,   9]],\n",
      "\n",
      "        [[ 85,  84,  83,  ..., 187, 170, 190],\n",
      "         [ 89,  87,  87,  ..., 187, 179, 180],\n",
      "         [ 92,  90,  87,  ..., 182, 175, 176],\n",
      "         ...,\n",
      "         [  3,   2,   3,  ...,   9,  11,   8],\n",
      "         [  0,   0,   1,  ...,   6,  11,   8],\n",
      "         [  2,   1,   4,  ...,   7,   6,   6]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "img_tensor = transform(image)\n",
    "print(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_preprocess = transforms.Compose([\n",
    "                    transforms.Resize((384, 384), interpolation=Image.BICUBIC),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#model\n",
    "model = models_mae.__dict__[args.model](norm_pix_loss=False, text_max_length=args.target_txt_len)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "checkpoint = torch.load(args.checkpoint_path, map_location='cpu') #why cpu?\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "#tokinizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "if args.dataset==\"ImageNet\":\n",
    "    dataset_train = datasets.ImageFolder(os.path.join(args.data_path, args.partition), transform=transform_test)\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train, num_samples=args.nb_eval)\n",
    "    lengths, dic2, mask_ = labels_lengths_with_BOS(tokenizer, \"labels/imagenet_labels.json\", dataset=args.dataset)\n",
    "elif args.dataset==\"CIFAR10\":\n",
    "    dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_test)\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train, num_samples=args.nb_eval)\n",
    "    lengths, dic2, mask_ = labels_lengths_with_BOS(tokenizer, \"labels/cifar10_labels.json\", dataset=args.dataset)\n",
    "elif args.dataset==\"CIFAR100\":\n",
    "    dataset_train = torchvision.datasets.CIFAR100(root='./data', train=True,download=True, transform=transform_test)\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train, num_samples=args.nb_eval)\n",
    "    lengths, dic2, mask_ = labels_lengths_with_BOS(tokenizer, \"labels/cifar100_labels.json\", dataset=args.dataset)\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, sampler=sampler_train,\n",
    "    batch_size=1,\n",
    "    num_workers=10,\n",
    "    pin_memory=\"store_true\",\n",
    "    drop_last=True,\n",
    ")\n",
    "proportions = {'1': 0, '2': 0}\n",
    "\n",
    "good=0\n",
    "count = 0\n",
    "text = ['this is a photo of a ']\n",
    "texts_tokenized_input = tokenizer.batch_encode_plus(text,add_special_tokens=True,return_tensors='pt')\n",
    "input_ids_ = texts_tokenized_input['input_ids'].to(device)\n",
    "input_ids_ = input_ids_[0][:-1].unsqueeze(0)\n",
    "for (image,label) in tqdm(data_loader_train):\n",
    "    res = -1\n",
    "    loss_res = 1000000\n",
    "    image = image.squeeze(0).to(device)\n",
    "    latent, _, _, _, _ = model.forward_encoder(image.unsqueeze(0), 0)\n",
    "    # decoder\n",
    "    x = model.decoder_embed(latent)\n",
    "    #Adding Image Posiotion embedding\n",
    "    x = x + model.decoder_pos_embed\n",
    "    x = x + model.decoder_image_type_embedding\n",
    "    key_padding_mask=torch.zeros(x.shape[0],x.shape[1],dtype=torch.bool, device=device)#useless\n",
    "    # x = rearrange(x, 'b n c->n b c') \n",
    "    for key, values in dic2.items():\n",
    "        for s in values:\n",
    "            #texts = torch.Tensor([s]).to(device).long()\n",
    "            texts = torch.Tensor([input_ids_[0].tolist()+s[1:]]).to(device).long()\n",
    "            t = model.text_embedding_layer_SimVLM(texts)\n",
    "            t = t + model.text_decoder_pos_embed[:, :t.shape[1], :]\n",
    "            t = t + model.decoder_text_type_embedding\n",
    "            target_text_length=len(texts[0])\n",
    "            tgt_mask = generate_square_subsequent_mask(target_text_length).to(device)\n",
    "            tgt_key_padding_mask=texts==0#shouldnt be used bause we do not use padding tokens\n",
    "            # t = rearrange(t, 'b n c->n b c')\n",
    "            output = model.decoder(t, x, is_causal=1, \n",
    "                                    tgt_mask=tgt_mask, memory_mask=None,\n",
    "                                    tgt_key_padding_mask=tgt_key_padding_mask,memory_key_padding_mask=key_padding_mask)\n",
    "            output = model.decoder_pred_text(output)\n",
    "            # output = rearrange(output, 'n b c -> b c n')\n",
    "            output = rearrange(output, 'b n c -> b c n')\n",
    "            target_text = texts[:,1:]\n",
    "            loss=F.cross_entropy(output[:,:,:-1],target_text)\n",
    "            if loss.item() < loss_res:\n",
    "                loss_res = loss.item()\n",
    "                res = int(key)\n",
    "    if res ==label.item():\n",
    "        good+=1\n",
    "    count+=1\n",
    "print(good/count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghostclipping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
